{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "from __future__ import print_function\n",
      "\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.preprocessing import Normalizer\n",
      "from sklearn import metrics\n",
      "\n",
      "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
      "\n",
      "import logging\n",
      "from optparse import OptionParser\n",
      "import sys\n",
      "from time import time\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "# jieba\n",
      "import os\n",
      "import jieba\n",
      "import chardet\n",
      "import jieba.analyse\n",
      "import jieba.posseg as pseg\n",
      "\n",
      "# load text files\n",
      "import os\n",
      "import chardet\n",
      "\n",
      "\n",
      "# Define News class\n",
      "\n",
      "class New:\n",
      "    def __init__(self, number, title, time, category, content, url):\n",
      "        self.number = number\n",
      "        self.title = title\n",
      "        self.time = time\n",
      "        self.category = category\n",
      "        self.content = content\n",
      "        self.url = url\n",
      "        self.similarity = []\n",
      "        \n",
      "\n",
      "# The apart text of corpus\n",
      "stop_words = []\n",
      "file = open(\"stopword\" ,\"rb\")\n",
      "for line in file:\n",
      "    stop_words.append(line[:len(line)-1])\n",
      "    \n",
      "# print stop_words\n",
      "def tokenizer(stop_words,s):\n",
      "    from nltk.tokenize import WhitespaceTokenizer\n",
      "    tok = s.split(\" \")\n",
      "    # print stop_words\n",
      "    # remove stop words\n",
      "    result = \"\"\n",
      "    for w in tok:\n",
      "        if w.strip() not in stop_words and w.strip() != \"\":\n",
      "            result += w.strip()\n",
      "            result += \" \"\n",
      "            \n",
      "    return result\n",
      "\n",
      "jieba.load_userdict(\"dict.txt\")\n",
      "path = \"fat_news/\"\n",
      "corpus = []\n",
      "\n",
      "news = []\n",
      "\n",
      "for root, dirs, files in os.walk(path):\n",
      "    # print root\n",
      "    for j,f in enumerate(files):\n",
      "        news.append([])\n",
      "        print(\"================================   \" +str(j)+ \"   ================================\")\n",
      "        doc = \"\"\n",
      "        date = 0\n",
      "        url = \"\"\n",
      "        title = \"\"\n",
      "\n",
      "        file_path = os.path.join(root, f)\n",
      "        with open( file_path ) as op:\n",
      "            for i,line in enumerate(op):\n",
      "                if i is 0:\n",
      "                    url = line\n",
      "                elif i is 2:\n",
      "                    date = line # use later\n",
      "                elif i is 4:\n",
      "                    title = line\n",
      "                else:\n",
      "                    doc += line\n",
      "        news[j] = New(j, title, date, -1, doc, url)\n",
      "\n",
      "\n",
      "        fileread = lambda filename: open(filename, \"rb\").read()\n",
      "        coding = chardet.detect(fileread(file_path))['encoding']\n",
      "\t\t# {'confidence': 0.99, 'encoding': 'utf-8'}\n",
      "        news[j].content = news[j].content.decode( coding )\n",
      "\n",
      "        \n",
      "        seg_list = jieba.cut_for_search(news[j].content)  # \u641c\u7d22\u5f15\u64ce\u6a21\u5f0f\n",
      "        result_list = \" \".join(seg_list).encode('utf-8')\n",
      "        result_list = tokenizer(stop_words,result_list)\n",
      "        corpus.append(result_list)\n",
      "        \n",
      "\n",
      "# print(corpus)\n",
      "\n",
      "# Display progress logs on stdout\n",
      "logging.basicConfig(level=logging.INFO,\n",
      "                    format='%(asctime)s %(levelname)s %(message)s')\n",
      "\n",
      "# __doc__ is a global variable that contains the documentation string of your script\n",
      "print(__doc__)\n",
      "\n",
      "from numpy import array\n",
      "\n",
      "n_digits = 3\n",
      "n_samples, n_features = (25,1927)\n",
      "labels = array([0,1,2,1,1,2,2,1,2,0,0,0,1,1,2,1,1,1,1,1,1,1,1,2,1])\n",
      "true_k = np.unique(labels).shape[0]\n",
      "\n",
      "# print(true_k)\n",
      "# print(data)\n",
      "\n",
      "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
      "t0 = time()\n",
      "\n",
      "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
      "\n",
      "\n",
      "X = vectorizer.fit_transform(corpus)\n",
      "\n",
      "print(\"done in %fs\" % (time() - t0))\n",
      "# n_samples: how many articles are there\n",
      "# n_features: how many different words in all articles are there\n",
      "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
      "print()\n",
      "\n",
      "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
      "\n",
      "print(\"Clustering sparse data with %s\" % km)\n",
      "t0 = time()\n",
      "km.fit(X)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "print()\n",
      "\n",
      "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
      "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
      "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
      "print(\"Adjusted Rand-Index: %.3f\"\n",
      "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
      "\n",
      "print(\"Silhouette Coefficient: %0.3f\"\n",
      "     % metrics.silhouette_score(X, labels, sample_size=None))\n",
      "\n",
      "print(labels)\n",
      "\n",
      "print()\n",
      "\n",
      "# if not (opts.n_components or opts.use_hashing):\n",
      "print(\"Top terms per cluster:\")\n",
      "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "terms = vectorizer.get_feature_names()\n",
      "for i in range(true_k):\n",
      "    print(\"Cluster %d:\" % i, end='')\n",
      "    for ind in order_centroids[i, :10]:\n",
      "        print(' %s' % terms[ind], end='')\n",
      "    print()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Building Trie..., from /Library/Python/2.7/site-packages/jieba/dict.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:jieba:Building Trie..., from /Library/Python/2.7/site-packages/jieba/dict.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "loading model from cache /var/folders/rm/rny1fml50cn19445xq9gcjm00000gn/T/jieba.cache\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:jieba:loading model from cache /var/folders/rm/rny1fml50cn19445xq9gcjm00000gn/T/jieba.cache\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "loading model cost 3.9634039402 seconds.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:jieba:loading model cost 3.9634039402 seconds.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Trie has been built succesfully.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:jieba:Trie has been built succesfully.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================   0   ================================\n",
        "================================   1   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   2   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   3   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   4   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   5   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   6   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   7   ================================\n",
        "================================   8   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   9   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   10   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   11   ================================\n",
        "================================   12   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   13   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   14   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   15   ================================\n",
        "================================   16   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   17   ================================\n",
        "================================   18   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   19   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   20   ================================\n",
        "================================   21   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   22   ================================\n",
        "================================   23   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   24   ================================\n",
        "Automatically created module for IPython interactive environment"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Extracting features from the training dataset using a sparse vectorizer\n",
        "done in 0.014058s\n",
        "n_samples: 25, n_features: 495\n",
        "\n",
        "Clustering sparse data with KMeans(copy_x=True, init=k-means++, max_iter=100, n_clusters=3, n_init=1,\n",
        "    n_jobs=1, precompute_distances=True, random_state=None, tol=0.0001,\n",
        "    verbose=0)\n",
        "done in 0.009s\n",
        "\n",
        "Homogeneity: 0.324\n",
        "Completeness: 0.324\n",
        "V-measure: 0.324\n",
        "Adjusted Rand-Index: 0.175\n",
        "Silhouette Coefficient: 0.030\n",
        "[0 1 2 1 1 2 2 1 2 0 0 0 1 1 2 1 1 1 1 1 1 1 1 2 1]\n",
        "\n",
        "Top terms per cluster:\n",
        "Cluster 0: \u9000\u8cbb \u6d88\u8cbb\u8005 \u5317\u5e02 \u767c\u7968 \u6c11\u773e \u63a5\u53d7 \u5317\u5e02\u5e9c \u5e02\u5e9c keith \u91d1\u984d\n",
        "Cluster 1: \u98df\u54c1 \u885b\u751f \u9175\u6bcd \u885b\u751f\u5c40 \u7a3d\u67e5 \u6dfb\u52a0\u7269 \u7db2\u53cb \u9999\u6e2f \u5316\u5b78 \u8d77\u8a34\n",
        "Cluster 2: \u6392\u968a \u9023\u52dd \u9023\u52dd\u6587 \u624b\u611f \u6c11\u773e \u9eb5\u5305\u5e97 \u8cb4\u8cd3 \u8cb4\u8cd3\u5361 \u8a31\u96c5\u921e \u7d42\u8eab\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(news)):\n",
      "    news[i].category = labels[i]\n",
      "\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "for i in range(len(news)):\n",
      "    news[i].similarity = cosine_similarity(X[i:i+1], X)[0]\n",
      "    cs = news[i].similarity\n",
      "\n",
      "# for i in range(len(news)):\n",
      "#     print(news[i].number, news[i].title, news[i].time, news[i].category, news[i].url, news[i].similarity)\n",
      "\n",
      "topNew_similarity = []\n",
      "cross = 0\n",
      "crosspoint = []\n",
      "k = len(news) # the similarity of first len(news) news would be the sam\n",
      "\n",
      "for i in range(len(news)):\n",
      "    topNew_similarity.extend(news[i].similarity)\n",
      "\n",
      "topNew_similarity = np.array(topNew_similarity)\n",
      "\n",
      "from heapq import nlargest\n",
      "while cross < 5: # control how many crosspoints \n",
      "    k = k+2 # it is doubled side\n",
      "    topk_similarity = nlargest(k, enumerate(topNew_similarity), key=lambda x: x[1])[-1]\n",
      "    \n",
      "#     print (topk_similarity)\n",
      "#     print (topk_similarity[0]/len(news)) # the article I belong\n",
      "#     print (topk_similarity[0]%len(news)) # the article I similar with\n",
      "#     print (labels[topk_similarity[0]/len(news)])\n",
      "#     print (labels[topk_similarity[0]%len(news)])\n",
      "    \n",
      "    if labels[topk_similarity[0]/len(news)] != labels[topk_similarity[0]%len(news)]:\n",
      "        cross += 1\n",
      "        crosspoint.append(topk_similarity)\n",
      "        \n",
      "        print (topk_similarity)\n",
      "        print (topk_similarity[0]/len(news)) # the article I belong\n",
      "        print (topk_similarity[0]%len(news)) # the article I similar with\n",
      "        print (labels[topk_similarity[0]/len(news)])\n",
      "        print (labels[topk_similarity[0]%len(news)])\n",
      "\n",
      "print (crosspoint)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(250, 0.58261886276272823)\n",
        "10\n",
        "0\n",
        "(551, 0.52110165559915433)\n",
        "22\n",
        "1\n",
        "(594, 0.49566900440504263)\n",
        "23\n",
        "19\n",
        "(352, 0.47951838756814419)\n",
        "14\n",
        "2\n",
        "(415, 0.42148182611330826)\n",
        "16\n",
        "15\n",
        "(566, 0.36814478474516094)\n",
        "22\n",
        "16\n",
        "(592, 0.35459415748214573)\n",
        "23\n",
        "17\n",
        "(492, 0.35176212526817158)\n",
        "19\n",
        "17\n",
        "(285, 0.34931607884578708)\n",
        "11\n",
        "10\n",
        "(176, 0.33228133368823121)\n",
        "7\n",
        "1\n",
        "(588, 0.32317396184256986)\n",
        "23\n",
        "13\n",
        "(205, 0.30873486114799431)\n",
        "8\n",
        "5\n",
        "(562, 0.30223682973982957)\n",
        "22\n",
        "12\n",
        "(327, 0.29529621928382055)\n",
        "13\n",
        "2\n",
        "(337, 0.27328507054045426)\n",
        "13\n",
        "12\n",
        "(401, 0.2646789104985503)\n",
        "16\n",
        "1\n",
        "(426, 0.25824771476382985)\n",
        "17\n",
        "1\n",
        "(601, 0.25515451143899881)\n",
        "24\n",
        "1\n",
        "(413, 0.254115238372081)\n",
        "16\n",
        "13\n",
        "(333, 0.2522982186124339)\n",
        "13\n",
        "8\n",
        "[(594, 0.49566900440504263), (592, 0.35459415748214573), (588, 0.32317396184256986), (327, 0.29529621928382055), (333, 0.2522982186124339)]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from heapq import nlargest\n",
      "from random import random\n",
      "s = time()\n",
      "nlargest(5, (random() for i in xrange(10000)))\n",
      "\n",
      "e = time()\n",
      "\n",
      "print (e-s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0054919719696\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}