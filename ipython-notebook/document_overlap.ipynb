{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "from __future__ import print_function\n",
      "\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.preprocessing import Normalizer\n",
      "from sklearn import metrics\n",
      "\n",
      "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
      "\n",
      "import logging\n",
      "from optparse import OptionParser\n",
      "import sys\n",
      "from time import time\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "# jieba\n",
      "import os\n",
      "import jieba\n",
      "import chardet\n",
      "import jieba.analyse\n",
      "import jieba.posseg as pseg\n",
      "\n",
      "# load text files\n",
      "import os\n",
      "import chardet\n",
      "\n",
      "\n",
      "# Define News class\n",
      "\n",
      "class New:\n",
      "    def __init__(self, number, title, time, category, content, url):\n",
      "        self.number = number\n",
      "        self.title = title\n",
      "        self.time = time\n",
      "        self.category = category\n",
      "        self.content = content\n",
      "        self.url = url\n",
      "        self.similarity = []\n",
      "        \n",
      "\n",
      "# The apart text of corpus\n",
      "stop_words = []\n",
      "file = open(\"stopword\" ,\"rb\")\n",
      "for line in file:\n",
      "    stop_words.append(line[:len(line)-1])\n",
      "    \n",
      "# print stop_words\n",
      "def tokenizer(stop_words,s):\n",
      "    from nltk.tokenize import WhitespaceTokenizer\n",
      "    tok = s.split(\" \")\n",
      "    # print stop_words\n",
      "    # remove stop words\n",
      "    result = \"\"\n",
      "    for w in tok:\n",
      "        if w.strip() not in stop_words and w.strip() != \"\":\n",
      "            result += w.strip()\n",
      "            result += \" \"\n",
      "            \n",
      "    return result\n",
      "\n",
      "jieba.load_userdict(\"dict.txt\")\n",
      "path = \"fat_news/\"\n",
      "corpus = []\n",
      "\n",
      "news = []\n",
      "\n",
      "for root, dirs, files in os.walk(path):\n",
      "    # print root\n",
      "    for j,f in enumerate(files):\n",
      "        news.append([])\n",
      "        print(\"================================   \" +str(j)+ \"   ================================\")\n",
      "        doc = \"\"\n",
      "        date = 0\n",
      "        url = \"\"\n",
      "        title = \"\"\n",
      "\n",
      "        file_path = os.path.join(root, f)\n",
      "        with open( file_path ) as op:\n",
      "            for i,line in enumerate(op):\n",
      "                if i is 0:\n",
      "                    url = line\n",
      "                elif i is 2:\n",
      "                    date = line # use later\n",
      "                elif i is 4:\n",
      "                    title = line\n",
      "                else:\n",
      "                    doc += line\n",
      "        news[j] = New(j, title, date, -1, doc, url)\n",
      "\n",
      "\n",
      "        fileread = lambda filename: open(filename, \"rb\").read()\n",
      "        coding = chardet.detect(fileread(file_path))['encoding']\n",
      "\t\t# {'confidence': 0.99, 'encoding': 'utf-8'}\n",
      "        news[j].content = news[j].content.decode( coding )\n",
      "\n",
      "        \n",
      "        seg_list = jieba.cut_for_search(news[j].content)  # \u641c\u7d22\u5f15\u64ce\u6a21\u5f0f\n",
      "        result_list = \" \".join(seg_list).encode('utf-8')\n",
      "        result_list = tokenizer(stop_words,result_list)\n",
      "        corpus.append(result_list)\n",
      "        \n",
      "\n",
      "# print(corpus)\n",
      "\n",
      "# Display progress logs on stdout\n",
      "logging.basicConfig(level=logging.INFO,\n",
      "                    format='%(asctime)s %(levelname)s %(message)s')\n",
      "\n",
      "# __doc__ is a global variable that contains the documentation string of your script\n",
      "print(__doc__)\n",
      "\n",
      "from numpy import array\n",
      "\n",
      "n_digits = 3\n",
      "n_samples, n_features = (25,1927)\n",
      "labels = array([0,1,2,1,1,2,2,1,2,0,0,0,1,1,2,1,1,1,1,1,1,1,1,2,1])\n",
      "true_k = np.unique(labels).shape[0]\n",
      "\n",
      "# print(true_k)\n",
      "# print(data)\n",
      "\n",
      "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
      "t0 = time()\n",
      "\n",
      "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
      "\n",
      "\n",
      "X = vectorizer.fit_transform(corpus)\n",
      "\n",
      "print(\"done in %fs\" % (time() - t0))\n",
      "# n_samples: how many articles are there\n",
      "# n_features: how many different words in all articles are there\n",
      "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
      "print()\n",
      "\n",
      "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
      "\n",
      "print(\"Clustering sparse data with %s\" % km)\n",
      "t0 = time()\n",
      "km.fit(X)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "print()\n",
      "\n",
      "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
      "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
      "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
      "print(\"Adjusted Rand-Index: %.3f\"\n",
      "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
      "\n",
      "print(\"Silhouette Coefficient: %0.3f\"\n",
      "     % metrics.silhouette_score(X, labels, sample_size=None))\n",
      "\n",
      "print(labels)\n",
      "\n",
      "print()\n",
      "\n",
      "# if not (opts.n_components or opts.use_hashing):\n",
      "print(\"Top terms per cluster:\")\n",
      "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "terms = vectorizer.get_feature_names()\n",
      "for i in range(true_k):\n",
      "    print(\"Cluster %d:\" % i, end='')\n",
      "    for ind in order_centroids[i, :10]:\n",
      "        print(' %s' % terms[ind], end='')\n",
      "    print()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Building Trie..., from /Library/Python/2.7/site-packages/jieba/dict.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:jieba:Building Trie..., from /Library/Python/2.7/site-packages/jieba/dict.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "loading model from cache /var/folders/rm/rny1fml50cn19445xq9gcjm00000gn/T/jieba.cache\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:jieba:loading model from cache /var/folders/rm/rny1fml50cn19445xq9gcjm00000gn/T/jieba.cache\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "loading model cost 3.9634039402 seconds.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:jieba:loading model cost 3.9634039402 seconds.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Trie has been built succesfully.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:jieba:Trie has been built succesfully.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================   0   ================================\n",
        "================================   1   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   2   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   3   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   4   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   5   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   6   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   7   ================================\n",
        "================================   8   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   9   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   10   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   11   ================================\n",
        "================================   12   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   13   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   14   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   15   ================================\n",
        "================================   16   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   17   ================================\n",
        "================================   18   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   19   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   20   ================================\n",
        "================================   21   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   22   ================================\n",
        "================================   23   ================================"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "================================   24   ================================\n",
        "Automatically created module for IPython interactive environment"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Extracting features from the training dataset using a sparse vectorizer\n",
        "done in 0.014058s\n",
        "n_samples: 25, n_features: 495\n",
        "\n",
        "Clustering sparse data with KMeans(copy_x=True, init=k-means++, max_iter=100, n_clusters=3, n_init=1,\n",
        "    n_jobs=1, precompute_distances=True, random_state=None, tol=0.0001,\n",
        "    verbose=0)\n",
        "done in 0.009s\n",
        "\n",
        "Homogeneity: 0.324\n",
        "Completeness: 0.324\n",
        "V-measure: 0.324\n",
        "Adjusted Rand-Index: 0.175\n",
        "Silhouette Coefficient: 0.030\n",
        "[0 1 2 1 1 2 2 1 2 0 0 0 1 1 2 1 1 1 1 1 1 1 1 2 1]\n",
        "\n",
        "Top terms per cluster:\n",
        "Cluster 0: \u9000\u8cbb \u6d88\u8cbb\u8005 \u5317\u5e02 \u767c\u7968 \u6c11\u773e \u63a5\u53d7 \u5317\u5e02\u5e9c \u5e02\u5e9c keith \u91d1\u984d\n",
        "Cluster 1: \u98df\u54c1 \u885b\u751f \u9175\u6bcd \u885b\u751f\u5c40 \u7a3d\u67e5 \u6dfb\u52a0\u7269 \u7db2\u53cb \u9999\u6e2f \u5316\u5b78 \u8d77\u8a34\n",
        "Cluster 2: \u6392\u968a \u9023\u52dd \u9023\u52dd\u6587 \u624b\u611f \u6c11\u773e \u9eb5\u5305\u5e97 \u8cb4\u8cd3 \u8cb4\u8cd3\u5361 \u8a31\u96c5\u921e \u7d42\u8eab\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Find Crosspoint"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u65b9\u6cd5\u4e00"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(news)):\n",
      "    news[i].category = labels[i]\n",
      "\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "for i in range(len(news)):\n",
      "    news[i].similarity = cosine_similarity(X[i:i+1], X)[0]\n",
      "    cs = news[i].similarity\n",
      "\n",
      "# for i in range(len(news)):\n",
      "#     print(news[i].number, news[i].title, news[i].time, news[i].category, news[i].url, news[i].similarity)\n",
      "\n",
      "from heapq import nlargest\n",
      "\n",
      "# ======================== overlap method 1 =========================================\n",
      "\n",
      "New_similarity = []\n",
      "cross = 0\n",
      "crosspoint = []\n",
      "k = len(news) # the similarity of first len(news) news would be the sam\n",
      "\n",
      "for i in range(len(news)):\n",
      "    New_similarity.extend(news[i].similarity)\n",
      "\n",
      "New_similarity = np.array(New_similarity)\n",
      "\n",
      "while cross < 5: # control how many crosspoints \n",
      "    k = k+2 # it is doubled side\n",
      "    topk_similarity = nlargest(k, enumerate(New_similarity), key=lambda x: x[1])[-1]\n",
      "    \n",
      "#     print (topk_similarity)\n",
      "#     print (topk_similarity[0]/len(news)) # the article I belong\n",
      "#     print (topk_similarity[0]%len(news)) # the article I similar with\n",
      "#     print (labels[topk_similarity[0]/len(news)])\n",
      "#     print (labels[topk_similarity[0]%len(news)])\n",
      "    \n",
      "    if labels[topk_similarity[0]/len(news)] != labels[topk_similarity[0]%len(news)]:\n",
      "        cross += 1\n",
      "        crosspoint.append(topk_similarity)\n",
      "        \n",
      "        print (topk_similarity)\n",
      "        print (news[(topk_similarity[0]/len(news))].content) # the article I belong\n",
      "        print (news[(topk_similarity[0]%len(news))].content) # the article I similar with\n",
      "        print (topk_similarity[0]/len(news))\n",
      "        print (topk_similarity[0]%len(news))\n",
      "\n",
      "print (crosspoint)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Consider of the Time ..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "from heapq import nlargest\n",
      "from random import random\n",
      "s = time()\n",
      "nlargest(5, (random() for i in xrange(10000)))\n",
      "\n",
      "e = time()\n",
      "\n",
      "print (e-s)\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "'\\nfrom heapq import nlargest\\nfrom random import random\\ns = time()\\nnlargest(5, (random() for i in xrange(10000)))\\n\\ne = time()\\n\\nprint (e-s)\\n'"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ======================== overlap method 2 =========================================\n",
      "\n",
      "topk_similarity_m2 = nlargest(100+len(news), enumerate(New_similarity), key=lambda x: x[1])\n",
      "crosspoint_m2 = []\n",
      "\n",
      "for sim in topk_similarity_m2[len(news):]:\n",
      "    crosspoint_m2.append((sim[0]/len(news),sim[0]%len(news)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u65b9\u6cd5\u4e8c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "from copy import copy\n",
      "from operator import itemgetter\n",
      "from heapq import heappush, heappop\n",
      "from collections import defaultdict\n",
      "from itertools import combinations # requires python 2.6+\n",
      "from optparse import OptionParser\n",
      "\n",
      "def swap(a,b):\n",
      "    if a > b:\n",
      "        return b,a\n",
      "    return a,b\n",
      "\n",
      "nodetype=str\n",
      "adj = defaultdict(set)\n",
      "edges = set()\n",
      "for line in crosspoint_m2:\n",
      "    L = line\n",
      "    ni,nj = nodetype(L[0]),nodetype(L[1]) # other columns ignored\n",
      "    if ni != nj: # skip any self-loops...\n",
      "        edges.add( swap(ni,nj) )\n",
      "        adj[ni].add(nj)\n",
      "        adj[nj].add(ni) # since undirected"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crosspoint_m2 = [('a','b'),('b','c'),('a','c')]\n",
      "crosspoint_m2 = [('a','b'),('b','c'),('a','c'),('d','e'),('e','f'),('d','f'),('c','e')]\n",
      "\n",
      "\n",
      "import sys, os\n",
      "from copy import copy\n",
      "from operator import itemgetter\n",
      "from heapq import heappush, heappop\n",
      "from collections import defaultdict\n",
      "from itertools import combinations # requires python 2.6+\n",
      "from optparse import OptionParser\n",
      "\n",
      "def swap(a,b):\n",
      "    if a > b:\n",
      "        return b,a\n",
      "    return a,b\n",
      "\n",
      "\n",
      "def Dc(m,n):\n",
      "    \"\"\"partition density\"\"\"\n",
      "    try:\n",
      "        return m*(m-n+1.0)/(n-2.0)/(n-1.0)\n",
      "    except ZeroDivisionError: # numerator is \"strongly zero\"\n",
      "        return 0.0\n",
      "\n",
      "\n",
      "class HLC:\n",
      "    def __init__(self,adj,edges):\n",
      "        self.adj   = adj\n",
      "        self.edges = edges\n",
      "        self.Mfactor  = 2.0 / len(edges)\n",
      "        self.edge2cid = {}\n",
      "        self.cid2nodes,self.cid2edges = {},{}\n",
      "        self.initialize_edges() # every edge in its own comm\n",
      "        self.D = 0.0 # partition density\n",
      "    \n",
      "    def initialize_edges(self):\n",
      "        for cid,edge in enumerate(self.edges):\n",
      "            edge = swap(*edge) # just in case\n",
      "            self.edge2cid[edge] = cid\n",
      "            self.cid2edges[cid] = set([edge])\n",
      "            self.cid2nodes[cid] = set( edge )\n",
      "    \n",
      "    def merge_comms(self,edge1,edge2):\n",
      "        cid1,cid2 = self.edge2cid[edge1],self.edge2cid[edge2]\n",
      "        if cid1 == cid2: # already merged!\n",
      "            return\n",
      "        m1,m2 = len(self.cid2edges[cid1]),len(self.cid2edges[cid2])\n",
      "        n1,n2 = len(self.cid2nodes[cid1]),len(self.cid2nodes[cid2])\n",
      "        Dc1, Dc2 = Dc(m1,n1), Dc(m2,n2)\n",
      "        if m2 > m1: # merge smaller into larger\n",
      "            cid1,cid2 = cid2,cid1\n",
      "        \n",
      "        self.cid2edges[cid1] |= self.cid2edges[cid2]\n",
      "        for e in self.cid2edges[cid2]: # move edges,nodes from cid2 to cid1\n",
      "            self.cid2nodes[cid1] |= set( e )\n",
      "            self.edge2cid[e] = cid1\n",
      "        del self.cid2edges[cid2], self.cid2nodes[cid2]\n",
      "        \n",
      "        m,n = len(self.cid2edges[cid1]),len(self.cid2nodes[cid1]) \n",
      "        Dc12 = Dc(m,n)\n",
      "        self.D = self.D + ( Dc12 -Dc1 - Dc2) * self.Mfactor # update partition density\n",
      "    \n",
      "    def single_linkage(self,threshold=None):\n",
      "        \"\"\"docstring goes here...\"\"\"\n",
      "        print(\"clustering...\")\n",
      "        self.list_D = [(1.0,0.0)] # list of (S_i,D_i) tuples...\n",
      "        self.best_D = 0.0\n",
      "        self.best_S = 1.0 # similarity threshold at best_D\n",
      "        self.best_P = None # best partition, dict: edge -> cid\n",
      "        \n",
      "        H = similarities( self.adj ) # min-heap ordered by 1-s\n",
      "        S_prev = -1\n",
      "        for oms,eij_eik in H:\n",
      "            S = 1-oms # remember, H is a min-heap\n",
      "            if S < threshold:\n",
      "                break\n",
      "                \n",
      "            if S != S_prev: # update list\n",
      "                if self.D >= self.best_D: # check PREVIOUS merger, because that's\n",
      "                    self.best_D = self.D  # the end of the tie\n",
      "                    self.best_S = S\n",
      "                    self.best_P = copy(self.edge2cid) # slow...\n",
      "                self.list_D.append( (S,self.D) )\n",
      "                S_prev = S\n",
      "            self.merge_comms( *eij_eik )\n",
      "        \n",
      "        self.list_D.append( (0.0,self.list_D[-1][1]) ) # add final val\n",
      "        if threshold != None:\n",
      "            return self.edge2cid, self.D\n",
      "        return self.best_P, self.best_S, self.best_D, self.list_D\n",
      "    \n",
      "\n",
      "def similarities(adj):\n",
      "    \"\"\"Get all the edge similarities. Input dict maps nodes to sets of neighbors.\n",
      "    Output is a list of decorated edge-pairs, (1-sim,eij,eik), ordered by similarity.\n",
      "    \"\"\"\n",
      "    print(\"computing similarities...\")\n",
      "    i_adj = dict( (n,adj[n] | set([n])) for n in adj)  # node -> inclusive neighbors\n",
      "    min_heap = [] # elements are (1-sim,eij,eik)\n",
      "    for n in adj: # n is the shared node\n",
      "        if len(adj[n]) > 1:\n",
      "            for i,j in combinations(adj[n],2): # all unordered pairs of neighbors\n",
      "                edge_pair = swap( swap(i,n),swap(j,n) )\n",
      "                inc_ns_i,inc_ns_j = i_adj[i],i_adj[j] # inclusive neighbors\n",
      "                S = 1.0 * len(inc_ns_i&inc_ns_j) / len(inc_ns_i|inc_ns_j) # Jacc similarity...\n",
      "                heappush( min_heap, (1-S,edge_pair) )\n",
      "    return [ heappop(min_heap) for i in xrange(len(min_heap)) ] # return ordered edge pairs\n",
      "\n",
      "\n",
      "def read_edgelist(nodetype=str):\n",
      "    adj = defaultdict(set)\n",
      "    edges = set()\n",
      "    for line in crosspoint_m2:\n",
      "        L = line\n",
      "        ni,nj = nodetype(L[0]),nodetype(L[1]) # other columns ignored\n",
      "        if ni != nj: # skip any self-loops...\n",
      "            edges.add( swap(ni,nj) )\n",
      "            adj[ni].add(nj)\n",
      "            adj[nj].add(ni) # since undirected\n",
      "    return dict(adj), edges\n",
      "\n",
      "\n",
      "def write_edge2cid(e2c,delimiter=\",\"):\n",
      "    # write edge2cid three-column file\n",
      "    c2c = dict( (c,i+1) for i,c in enumerate(sorted(list(set(e2c.values())))) ) # ugly...\n",
      "    for e,c in sorted(e2c.iteritems(), key=itemgetter(1)):\n",
      "        print ( \"%s%s%s%s%s\\n\" % (str(e[0]),delimiter,str(e[1]),delimiter,str(c2c[c])) )\n",
      "    \n",
      "    cid2edges,cid2nodes = defaultdict(set),defaultdict(set) # faster to recreate here than\n",
      "    for edge,cid in e2c.iteritems():                        # to keep copying all dicts\n",
      "        cid2edges[cid].add( edge )                          # during the linkage...\n",
      "        cid2nodes[cid] |= set(edge)\n",
      "    cid2edges,cid2nodes = dict(cid2edges),dict(cid2nodes)\n",
      "    \n",
      "    # write list of edges for each comm, each comm on its own line\n",
      "    for cid in sorted(cid2edges.keys()):\n",
      "        nodes,edges = map(str,cid2nodes[cid]), [\"%s,%s\" % (ni,nj) for ni,nj in cid2edges[cid]]\n",
      "        print ( \" \".join(edges) );\n",
      "        print ( \" \".join([str(cid)] + nodes) );\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "\n",
      "    print(\"# loading input...\")\n",
      "    adj,edges = read_edgelist()\n",
      "    \n",
      "    edge2cid,S_max,D_max,list_D = HLC( adj,edges ).single_linkage()\n",
      "    for s,D in list_D:\n",
      "        print(s, D)\n",
      "    print(\"# D_max = %f\\n# S_max = %f\" % (D_max,S_max))\n",
      "    write_edge2cid( edge2cid )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# loading input...\n",
        "clustering...\n",
        "computing similarities...\n",
        "1.0 0.0\n",
        "1.0 0.0\n",
        "0.75 0.0\n",
        "0.166666666667 0.857142857143\n",
        "0.0 0.857142857143\n",
        "# D_max = 0.857143\n",
        "# S_max = 0.166667\n",
        "e,f,1\n",
        "\n",
        "d,f,1\n",
        "\n",
        "d,e,1\n",
        "\n",
        "c,e,2\n",
        "\n",
        "a,b,3\n",
        "\n",
        "b,c,3\n",
        "\n",
        "a,c,3\n",
        "\n",
        "e,f d,f d,e\n",
        "3 e d f\n",
        "c,e\n",
        "4 c e\n",
        "a,b b,c a,c\n",
        "6 a c b\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}